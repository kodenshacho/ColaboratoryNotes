{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjkX+//YoSpvaUu6nosiSJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodenshacho/ColaboratoryNotes/blob/main/%E7%B5%B1%E8%A8%88%E3%81%A7%E6%A4%9C%E7%9F%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "X線の強度データを使って元素を検出するには、様々な手法があります。 ここでは、教師あり学習を使った手法を紹介します。\n",
        "\n",
        "教師あり学習では、あらかじめ正解データを与えて、それを使ってモデルを訓練します。 具体的には、次のような手順で行うことができます。\n",
        "\n",
        "データを収集する: X線の強度データと、それがどの元素に対応するかを知るための正解データを収集します。\n",
        "\n",
        "データを前処理する: 収集したデータを加工し、モデルに入力できるように整形します。"
      ],
      "metadata": {
        "id": "UecLsm0AWSYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルを選択する: どのような機械学習モデルを使うかを選択します。 例えば、X線強度データを分類する場合は、サポートベクターマシン (SVM) やランダムフォレストなどが使えます。\n",
        "\n",
        "モデルを訓練する: 前処理したデータを使って、選択したモデルを訓練します。\n",
        "\n",
        "モデルを評価する: 訓練したモデルが、未知のデータに対してどの程度正確に予測できるかを評価します。"
      ],
      "metadata": {
        "id": "Vksqo083Wazc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "このコードでは、X線の強度データをスケーリングしてから、訓練データとテストデータに分割します。 次に、サポートベクターマシンを使ってモデルを訓練し、テストデータでモデルの精度を評価しています。\n",
        "\n",
        "\n",
        "以上が、X線の強度データを使って元素を検出する方法の一例です。 実際には、複数のモデルを試してみたり、ハイパーパラメータを調整したりすることで、より高い精度を得ることができるでしょう。"
      ],
      "metadata": {
        "id": "D5LWN-vFW6Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 必要なライブラリをインポートする\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# X線の強度データと正解データを収集する\n",
        "X = # X線の強度データを表すnumpy array\n",
        "y = # 各データがどの元素に対応するかを表すnumpy array\n",
        "\n",
        "# データを前処理する\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# データを訓練データとテストデータに分割する\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
        "\n",
        "# サポートベクターマシンを使ってモデルを訓練する\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# モデルを評価する\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "lKoK6eGwW91R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "SVCでは、SVMの手法を用いて、複数のクラスに分類する問題を解くことができます。 具体的には、SVCは、以下のような手順で処理を行います。\n",
        "\n",
        "トレーニングセットを受け取る: SVCは、訓練データと正解ラベルを受け取ります。\n",
        "\n",
        "訓練データは、各データを表す特徴量の組を表すnumpy arrayで、正解ラベルは、それぞれのデータがどのクラスに属するかを表すnumpy arrayです。\n",
        "\n",
        "\n",
        "最適な分離平面を求める: SVCは、訓練データを二次元平面上にプロットし、それらを最もよく分離する直線を求めます。\n",
        "\n",
        "分類器を作成する: SVCは、得られた分離平面を使って、新しいデータを分類するための分類器を作成します。\n",
        "\n",
        "分類器を使って予測する: SVCが作成した分類器を使って、新しいデータを分類することができます。\n",
        "\n",
        "SVCは、汎化性能が高く、様々な分類問題に適用できるため、非常によく使われる手法です。 しかし、データが大規模であると、計算時間が非常に長くなることがあるため、注意が必要です。"
      ],
      "metadata": {
        "id": "IB-8WkMMXzPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Generate fake data with three peaks\n",
        "np.random.seed(0)\n",
        "data = np.concatenate([\n",
        "    np.random.normal(0, 0.5, size=50),\n",
        "    np.random.normal(5, 1, size=50),\n",
        "    np.random.normal(10, 0.5, size=50)\n",
        "])\n",
        "print(data)\n",
        "# Save the fake data to a text file\n",
        "np.savetxt('spectroscopy_data.txt', data, fmt='%.3f')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgThGgOJrWD0",
        "outputId": "b6a8ff05-d25e-4765-ed61-8cffb9306a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.88202617  0.2000786   0.48936899  1.1204466   0.933779   -0.48863894\n",
            "  0.47504421 -0.0756786  -0.05160943  0.20529925  0.07202179  0.72713675\n",
            "  0.38051886  0.06083751  0.22193162  0.16683716  0.74703954 -0.10257913\n",
            "  0.15653385 -0.42704787 -1.27649491  0.3268093   0.4322181  -0.37108251\n",
            "  1.13487731 -0.72718284  0.02287926 -0.09359193  0.76638961  0.73467938\n",
            "  0.07747371  0.18908126 -0.44389287 -0.99039823 -0.17395607  0.07817448\n",
            "  0.61514534  0.60118992 -0.19366341 -0.15115138 -0.52427648 -0.71000897\n",
            " -0.8531351   0.9753877  -0.25482609 -0.21903715 -0.62639768  0.38874518\n",
            " -0.80694892 -0.10637014  4.10453344  5.3869025   4.48919486  3.81936782\n",
            "  4.97181777  5.42833187  5.06651722  5.3024719   4.36567791  4.63725883\n",
            "  4.32753955  4.64044684  4.18685372  3.2737174   5.17742614  4.59821906\n",
            "  3.36980165  5.46278226  4.09270164  5.0519454   5.72909056  5.12898291\n",
            "  6.13940068  3.76517418  5.40234164  4.31518991  4.12920285  4.42115034\n",
            "  4.68844747  5.05616534  3.83485016  5.90082649  5.46566244  3.46375631\n",
            "  6.48825219  6.89588918  6.17877957  4.82007516  3.92924738  6.05445173\n",
            "  4.59682305  6.22244507  5.20827498  5.97663904  5.3563664   5.70657317\n",
            "  5.01050002  6.78587049  5.12691209  5.40198936 10.94157535  9.32612047\n",
            "  9.3647575  10.48469835  9.4134383  10.97181059  9.79319051  9.62627259\n",
            " 10.96147101 10.7402574  10.93377948 10.45302233  9.56938716 10.95503248\n",
            "  9.86599831 10.4012282  10.47362598  9.92249495 10.30703969 10.46110334\n",
            " 10.18821277  9.4502996  10.14911909 10.66319295  9.65271607  9.92518273\n",
            "  9.78242322 10.92463186 10.33614738 10.20373092  9.61504196 10.2696246\n",
            "  9.66283367 10.01591528  9.68207696 10.33821665 10.28829541  9.89585062\n",
            " 10.19800336  9.45346925  9.2543712  10.21969585 10.08333675 10.31751572\n",
            " 11.19157239 10.47223974  9.54358889 10.55850814  9.34204629  9.7692077 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "このコードでは、peaks、widths、heights の 3 つの配列を使用して、5 つのピークを生成しています。その後、NumPy の linspace 関数を使用して、0 から 30 までの範囲を 300 分割した x 配列を生成します。最後に、x 配列を使用して、y 配列を計算し、データを返します"
      ],
      "metadata": {
        "id": "4sObPbxss3ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def get_data():\n",
        "    # ピーク位置を生成する\n",
        "    peaks = np.array([5, 10, 15, 20, 25])\n",
        "\n",
        "    # ピーク幅を生成する\n",
        "    widths = np.array([1, 2, 1, 2, 1])\n",
        "\n",
        "    # ピーク強度を生成する\n",
        "    heights = np.array([10, 15, 10, 15, 10])\n",
        "\n",
        "    # ピークを使用してデータを生成する\n",
        "    x = np.linspace(0, 30, 300)\n",
        "    y = np.zeros_like(x)\n",
        "    for peak, width, height in zip(peaks, widths, heights):\n",
        "        y += height * np.exp(-((x - peak) / width) ** 2)\n",
        "\n",
        "    # データを返す\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "e93KP-Rys8zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このコードでは、sklearn モジュールの GaussianMixture クラスを使用して、エネルギー値と対応する強度データから 5 つのコンポーネントを持つガウス混合モデルを構築します。その後、fit メソッドを使用してモデルを訓練し、means_ プロパティからピークを推定します。最後に、matplotlib モジュールを使用して、エネルギー値と強度データをプロットし、推定されたピークを \"x\" マークでプロットします。"
      ],
      "metadata": {
        "id": "CYIIJw_usROY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def get_data():\n",
        "    # ピーク位置を生成する\n",
        "    peaks = np.array([5, 10, 15, 20, 25])\n",
        "\n",
        "    # ピーク幅を生成する\n",
        "    widths = np.array([1, 2, 1, 2, 1])\n",
        "\n",
        "    # ピーク強度を生成する\n",
        "    heights = np.array([10, 15, 10, 15, 10])\n",
        "\n",
        "    # ピークを使用してデータを生成する\n",
        "    x = np.linspace(0, 30, 300)\n",
        "    y = np.zeros_like(x)\n",
        "    for peak, width, height in zip(peaks, widths, heights):\n",
        "        y += height * np.exp(-((x - peak) / width) ** 2)\n",
        "\n",
        "    # データを2次元の配列に変換する\n",
        "    data = np.array([x, y]).T\n",
        "\n",
        "    # データを返す\n",
        "    return data"
      ],
      "metadata": {
        "id": "f9KN5lizupee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "energies = [0.5, 1.0, 1.5, 2.0, 2.5]\n",
        "intensities = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "gmm = GaussianMixture(n_components=5)\n",
        "gmm.fit(energies, intensities)\n",
        "\n",
        "peaks = gmm.means_\n",
        "\n",
        "plt.plot(energies, intensities)\n",
        "plt.plot(peaks, \"x\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8W3-88cLvadE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}